# dat_transformer
DAT (Dynamic Adaptive Transformer)

```bash
     pip install -r requirements.txt
```
----

### 1. Динамическая и Адаптивная Архитектура

**Текущая проблема:** Современный Трансформер — это монолитная, статичная структура. Он применяет одинаковое количество вычислений (например, 48 слоёв по 8 "головок" внимания) к каждому токену, будь то простой союз "и" или сложное понятие "эпистемология". Это всё равно что использовать суперкомпьютер для вычисления `2+2`. Это неэффективно и вычислительно расточительно.

С идеальным оптимизатором, который не требует гладких, дифференцируемых путей для обучения, мы могли бы реализовать следующие улучшения:

#### **1.1 Адаптивная глубина вычислений (Adaptive Depth)**
* **Механизм:** После каждого слоя `EncoderLayer` или `DecoderLayer` к каждому вектору токена применяется небольшая "вентильная" нейросеть (gating network). Эта сеть принимает на вход текущий вектор и решает, достаточно ли он обработан. Если да, она выводит сигнал "стоп", и этот вектор "пропускает" оставшиеся слои и сразу отправляется на выход стека. Если нет, вектор идёт на следующий слой для дальнейшей обработки.
* **Пример:** В предложении "Мама мыла раму" векторы для всех трёх слов могут быть полностью обработаны за 3-4 слоя. В предложении "Квантово-механическая интерпретация копенгагенской школы..." вектор для "интерпретация" может потребовать прохода через все 48 слоёв, чтобы полностью насытиться контекстом от окружающих его сложных терминов.
* **Результат:** Радикальное (возможно, в 5-10 раз) сокращение среднего количества вычислений на токен во время рабочего хода (inference) без потери качества для сложных случаев.

#### **1.2 Адаптивная ширина внимания (Adaptive Width)**
* **Механизм:** Аналогично адаптивной глубине, но внутри самого блока `Multi-Head Attention`. Перед основным расчётом внимания специальная "вентильная" сеть анализирует `Q`-вектор токена и решает, какие из `h` "головок" (специалистов) необходимо задействовать для данного конкретного запроса. Головки, которые не нужны, просто не выполняют свою дорогостоящую операцию `Q @ K^T`.
* **Пример:** Для разрешения местоимения "она" может быть активирована только одна головка, специализированная на поиске связей "местоимение-существительное". Для анализа простого слова "на" может быть достаточно одной-двух синтаксических головок.
* **Результат:** Дополнительное сокращение вычислений и, возможно, улучшение качества за счёт снижения "шума" от нерелевантных головок внимания.

---
### 2. Усложнение Механизма Внимания

**Текущая проблема:** `Scaled Dot-Product Attention` — это, по сути, простое скалярное произведение, мера линейного сходства. Этот метод был выбран за его вычислительную простоту и удобство для дифференцирования, а не за его предельную выразительную мощность. Он может упускать сложные, нелинейные взаимосвязи между понятиями.

С идеальным оптимизатором мы могли бы заменить его на гораздо более мощные функции.

#### **2.1 Параметрическое нелинейное внимание**
* **Механизм:** Вместо формулы `score = Q_i @ K_j^T` мы используем небольшую, но полноценную нейросеть (например, двухслойный перцептрон), которая принимает на вход оба вектора и выдаёт оценку их совместимости: `score = MLP(Concat(Q_i, K_j))`.
* **Пример:** Такая нейросеть могла бы выучить не просто то, что "король" и "королева" похожи, но и то, что они находятся в специфическом отношении "противоположный пол". Она могла бы научиться распознавать логические отношения (причина-следствие, часть-целое), которые простое скалярное произведение уловить не может.
* **Результат:** Значительное увеличение способности модели понимать тонкие и сложные семантические взаимосвязи.

---
### 3. Глубокая Интеграция с Внешней Памятью

**Текущая проблема:** Память Трансформера ограничена его контекстным окном. Он не может "помнить" информацию из предыдущих документов или даже из начала очень длинного документа.

Идеальный оптимизатор позволил бы нам эффективно обучать модели, которые могут выполнять дискретные операции чтения/записи в огромную внешнюю базу данных (векторную память).

#### **3.1 Внимание с доступом к памяти (Memory-Augmented Attention)**
* **Механизм:** `Q`-вектор каждого слова используется для поиска не только среди `K`-векторов текущего контекста, но и для выполнения запроса к огромной внешней векторной базе данных (памяти). Самые релевантные векторы, найденные в этой базе, извлекаются и добавляются в текущий контекст как дополнительные `K` и `V`.
* **Пример:** При обработке медицинского документа модель встречает название редкого лекарства. Её `Q`-вектор делает запрос к внешней памяти (которая содержит, например, всю Википедию или базу медицинских знаний) и "подтягивает" в текущий контекст вектор, описывающий состав и действие этого лекарства.
* **Результат:** Практически неограниченный контекст. Модель может использовать знания, полученные из миллионов документов, для обработки одного конкретного предложения.

#### **3.2 Обучаемая запись в память**
* **Механизм:** В архитектуру добавляются специальные "write-heads" (записывающие головки), которые после обработки блока текста (например, абзаца) решают, какую информацию стоит сохранить в памяти. Они генерируют вектор-ключ (например, на основе ключевых понятий) и вектор-значение (семантическая выжимка абзаца) и отправляют их в базу данных.
* **Пример:** Прочитав биографию Эйнштейна, модель сохраняет в память вектор "Эйнштейн" как ключ и вектор "физик, теория относительности, E=mc^2" как значение.
* **Результат:** Способность к непрерывному обучению и накоплению знаний. Модель не просто обрабатывает текст, она строит собственную базу знаний.

----

### 1\. Базовые компоненты

```pseudocode
// --- 1.1 Вентильная функция (Gating Function) ---
// Принимает вектор и решает, насколько "активировать" путь (возвращает число от 0 до 1)
function Gating(x, W_g, b_g):
    // @ - матричное умножение
    // sigmoid - сигмоидная функция
    return sigmoid(x @ W_g + b_g)

// --- 1.2 Параметрическая функция оценки внимания ---
// Принимает два вектора (Q и K) и вычисляет их совместимость с помощью нейросети
function ParametricScore(q_vector, k_vector, score_mlp):
    // Объединяем векторы в один
    concatenated = concatenate(q_vector, k_vector)
    // Пропускаем через нейросеть (MLP) для получения оценки
    score = score_mlp(concatenated)
    return score

// --- 1.3 Механизм доступа к памяти ---
// Находит k наиболее релевантных векторов в памяти для набора запросов Q
function RetrieveFromMemory(Q, M_K, M_V, k):
    // Находим совместимость каждого запроса с каждым ключом в памяти
    memory_scores = Q @ transpose(M_K)
    
    // Находим индексы k лучших ключей для каждого запроса
    top_k_indices = find_top_k_indices(memory_scores, k)
    
    // Извлекаем соответствующие ключи и значения
    K_mem = M_K[top_k_indices]
    V_mem = M_V[top_k_indices]
    
    return K_mem, V_mem
```

-----

### 2\. Улучшенные Архитектурные Блоки

```pseudocode
// --- 2.1 Внимание с доступом к памяти (Memory-Augmented Attention) ---
function MemoryAugmentedAttention(Q, K_ctx, V_ctx, M_K, M_V, k, score_mlp):
    // 1. Извлекаем релевантную информацию из внешней памяти
    K_mem, V_mem = RetrieveFromMemory(Q, M_K, M_V, k)
    
    // 2. Дополняем текущий контекст информацией из памяти
    K_aug = concatenate(K_ctx, K_mem)
    V_aug = concatenate(V_ctx, V_mem)
    
    // 3. Вычисляем оценки внимания с помощью параметрической функции
    // (вместо простого Q @ K^T)
    scores = create_empty_matrix()
    for i in 1 to number_of_queries:
        for j in 1 to number_of_keys:
            scores[i, j] = ParametricScore(Q[i], K_aug[j], score_mlp)
    
    // 4. Стандартное завершение
    weights = softmax(scores / sqrt(dimension_of_K))
    output = weights @ V_aug
    return output

// --- 2.2 Многоголовочное внимание с адаптивной шириной ---
function AdaptiveWidthMHA(X, num_heads, memory_params...):
    head_outputs_weighted = []
    
    // Анализируем вход, чтобы определить важность каждой головки
    head_importance_gates = Gating(average(X), W_gate, b_gate) // Вентиль для всех головок
    
    for i in 1 to num_heads:
        W_Q_i, W_K_i, W_V_i = get_weights_for_head(i)
        Q_i = X @ W_Q_i
        K_i = X @ W_K_i
        V_i = X @ W_V_i
        
        // Вычисляем выход головки (здесь может быть обычное или MemoryAugmentedAttention)
        head_output_i = ScaledDotProductAttention(Q_i, K_i, V_i)
        
        // Взвешиваем выход головки в соответствии с её "важностью"
        gate_weight = head_importance_gates[i]
        add (gate_weight * head_output_i) to head_outputs_weighted
    
    // Объединяем взвешенные результаты
    concatenated = concatenate(head_outputs_weighted)
    final_output = concatenated @ W_O
    return final_output
```

-----

### 3\. Финальная Архитектура: Динамический Трансформер

```pseudocode
// --- 3.1 Адаптивный слой ---
function AdaptiveLayer(x_input, layer_params...):
    // Выполняем все вычисления внутри слоя (внимание, FFN, Add&Norm)
    // с использованием адаптивных/улучшенных блоков
    attention_output = AdaptiveWidthMHA(x_input, ...)
    x_after_attention = LayerNorm(x_input + attention_output)
    
    ffn_output = FeedForwardNetwork(x_after_attention)
    x_output = LayerNorm(x_after_attention + ffn_output)
    
    // Вычисляем вероятность остановиться на этом слое
    halting_probability = Gating(x_output, W_halt, b_halt)
    
    return x_output, halting_probability

// --- 3.2 Динамический стек слоёв (например, Кодировщик) ---
function DynamicEncoder(X, max_layers):
    layer_outputs = []
    layer_weights = []
    
    remaining_probability = 1.0
    x_current = X
    
    for i in 1 to max_layers:
        // Получаем выход слоя и вероятность остановки
        x_next, halt_prob = AdaptiveLayer(x_current, ...)
        
        // Вычисляем итоговый вес для выхода этого слоя
        layer_weight = remaining_probability * halt_prob
        add layer_weight to layer_weights
        add x_next to layer_outputs
        
        // Обновляем вероятность "дойти" до следующего слоя
        remaining_probability = remaining_probability * (1 - halt_prob)
        x_current = x_next
        
    // Финальный выход - это взвешенная сумма выходов всех слоёв
    final_output = sum(layer_weights[i] * layer_outputs[i] for i in 1..max_layers)
    
    return final_output
```